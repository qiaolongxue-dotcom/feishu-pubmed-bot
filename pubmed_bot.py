import requests
import datetime
import time
import json

# ================= é…ç½®åŒº (è¯·ä¿®æ”¹ä»¥ä¸‹ä¿¡æ¯) =================
# 1. é£ä¹¦æœºå™¨äººçš„ Webhook åœ°å€
# å¦‚ä½•è·å–ï¼šé£ä¹¦ç¾¤ -> è®¾ç½® -> ç¾¤æœºå™¨äºº -> æ·»åŠ  -> è‡ªå®šä¹‰æœºå™¨äºº -> å¤åˆ¶ Webhook åœ°å€
import os
FEISHU_WEBHOOK_URL = os.environ.get("FEISHU_WEBHOOK", "ä½ çš„é»˜è®¤å¤‡ç”¨åœ°å€")

# 2. æœç´¢å…³é”®è¯é…ç½®
# å°†å…³é”®è¯æ‹†åˆ†ä¸ºåˆ—è¡¨ï¼Œç”¨äºåç»­è®¡ç®—åŒ¹é…åº¦è¯„åˆ†
KEYWORDS_LIST = [
    "Smooth muscle",
    "Phenotypic switching",
    "Endothelial cells",
    "Erectile dysfunction",
    "Cerium oxide nanozymes",
    "Diabetes"
]
# è‡ªåŠ¨ç”Ÿæˆ PubMed æŸ¥è¯¢è¯­å¥ (é€»è¾‘ OR)
SEARCH_KEYWORD = "(" + " OR ".join([f'"{k}"' for k in KEYWORDS_LIST]) + ")"

# 3. é™åˆ¶é…ç½®
MAX_FETCH_RESULTS = 50   # æ¯æ¬¡APIè·å–çš„å€™é€‰æ± å¤§å° (å»ºè®®å¤§ä¸€äº›ï¼Œä»¥ä¾¿ç­›é€‰å‡ºé«˜åŒ¹é…åº¦çš„)
DAILY_LIMIT = 10         # æ¯æ—¥æœ€å¤§æ¨é€æ•°é‡
HISTORY_FILE = "pubmed_history.json"  # æœ¬åœ°å†å²è®°å½•æ–‡ä»¶

# 4. ç›®æ ‡æœŸåˆŠåˆ—è¡¨ (æ³Œå°¿å¤–ç§‘æ•™æˆå…³æ³¨çš„é«˜åˆ†/æ ¸å¿ƒæœŸåˆŠ)
TARGET_JOURNALS = [
    "CA-A Cancer Journal for Clinicians",
    "New England Journal of Medicine",
    "The Lancet",
    "British Medical Journal",
    "Journal of the American Medical Association",
    "Nature Medicine",
    "Science Translational Medicine",
    "Cell Reports Medicine",
    "Cell",
    "Molecular Cancer",
    "Annual Review of Immunology",
    "Journal of Hepatology",
    "Molecular Neurodegeneration",
    "Cellular & Molecular Immunology",
    "Experimental & Molecular Medicine",
    "Immunity",
    "Molecular Biomedicine",
    "Journal of Biomedical Science",
    "Intensive Care Medicine",
    "Journal of Clinical Oncology",
    "European Urology",
    "Gastroenterology",
    "Journal of Neuroinflammation",
    "Journal of Allergy & Clinical Immunology",
    "Clinical Reviews in Allergy & Immunology",
    "Genome Medicine",
    "Diabetologia",
    "Journal of Translational Medicine",
    "Science",
    "Nature",
    "Nature Communications",
    "Science Advances",
    "Journal of Advanced Research",
    "National Science Review",
    "BMC Medicine",
    "Military Medical Research",
    "Cell Death & Differentiation",
    "Cell Research",
    "Science Bulletin",
    "Asian Journal of Pharmaceutical Sciences",
    "Acta Pharmacologica Sinica",
    "Translational Neurodegeneration",
    "Chinese Medical Journal",
    "Phenomics",
    "Nature Reviews Urology",
    "European Urology",
    "Journal of Clinical Oncology",
    "Cell Death & Disease",
    "Clinical Cancer Research",
    "Oncogene",
    "American Journal of Pathology",
    "Journal of Urology",
    "British Journal of Cancer",
    "Apoptosis"
]

# ================= å·¥å…·å‡½æ•°åŒº =================
import xml.etree.ElementTree as ET
import os

def load_history():
    """è¯»å–å†å²è®°å½•ï¼Œå¤„ç†æ¯æ—¥é™é¢"""
    today_str = datetime.date.today().isoformat()
    default_history = {"date": today_str, "count": 0, "sent_ids": []}
    
    if not os.path.exists(HISTORY_FILE):
        return default_history
        
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = json.load(f)
            # å¦‚æœæ—¥æœŸä¸æ˜¯ä»Šå¤©ï¼Œé‡ç½®è®¡æ•°ï¼Œä½†ä¿ç•™æ—§IDåšå»é‡ï¼ˆå¯é€‰ï¼Œè¿™é‡Œä¸ºäº†ç®€å•åªåšæœ¬æ—¥å»é‡/è®¡æ•°é‡ç½®ï¼‰
            # ä¹Ÿå¯ä»¥é€‰æ‹©åšå…¨é‡å»é‡ï¼Œé˜²æ­¢éš”æ—¥é‡å¤æ¨èã€‚è¿™é‡Œç­–ç•¥æ˜¯ï¼šé•¿æœŸå»é‡ã€‚
            if history.get("date") != today_str:
                # æ–°çš„ä¸€å¤©ï¼Œé‡ç½®è®¡æ•°ï¼Œä¿ç•™sent_idsä»¥é˜²æ­¢é‡å¤æ¨èæ—§æ–‡
                history["date"] = today_str
                history["count"] = 0
                # å¦‚æœ sent_ids å¤ªå¤§å¯ä»¥æ¸…ç†ï¼Œè¿™é‡Œæš‚ä¸”ä¿ç•™
            return history
    except Exception:
        return default_history

def save_history(history):
    """ä¿å­˜å†å²è®°å½•"""
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(history, f, indent=2)
    except Exception as e:
        print(f"[Warning] æ— æ³•ä¿å­˜å†å²è®°å½•: {e}")


def search_pubmed(keyword, max_results=50):
    """
    åœ¨ PubMed æœç´¢å…³é”®è¯ï¼Œå¹¶é™å®šåœ¨ TARGET_JOURNALS å®šä¹‰çš„æœŸåˆŠèŒƒå›´å†…
    """
    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"

    # æ„å»ºæœŸåˆŠé™å®šæŸ¥è¯¢è¯­å¥: ("Journal A"[Journal] OR "Journal B"[Journal] ...)
    # æ³¨æ„ï¼šå¦‚æœæœŸåˆŠåˆ—è¡¨ä¸ºç©ºï¼Œåˆ™ä¸è¿›è¡Œç­›é€‰
    if TARGET_JOURNALS:
        journal_terms = [f'"{j}"[Journal]' for j in TARGET_JOURNALS]
        journal_query = " OR ".join(journal_terms)
        final_term = f"({keyword}) AND ({journal_query})"
    else:
        final_term = keyword

    print(f"ğŸ” æ­£åœ¨æ£€ç´¢ {len(TARGET_JOURNALS)} æœ¬æŒ‡å®šæœŸåˆŠ...")

    params = {
        "db": "pubmed",
        "term": final_term,
        "retmode": "json",
        "retmax": max_results,
        "sort": "date" # æŒ‰æ—¥æœŸæ’åº
    }
    
    try:
        # ä½¿ç”¨ POST è¯·æ±‚é˜²æ­¢ URL è¿‡é•¿
        response = requests.post(base_url, data=params, timeout=20)
        response.raise_for_status()
        data = response.json()
        id_list = data.get("esearchresult", {}).get("idlist", [])
        return id_list
    except Exception as e:
        print(f"[Error] PubMed æœç´¢å¤±è´¥: {e}")
        return []

def get_details_and_rank(id_list):
    """
    ä½¿ç”¨ efetch è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå«æ‘˜è¦ï¼‰ï¼Œæ ¹æ®å…³é”®è¯åŒ¹é…åº¦æ’åº
    """
    if not id_list:
        return []

    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    ids = ",".join(id_list)
    params = {
        "db": "pubmed",
        "id": ids,
        "retmode": "xml"  # è·å– XML ä»¥ä¾¿æå– Abstract
    }

    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡çŒ®è¯¦æƒ…å¹¶åˆ†æåŒ¹é…åº¦...")

    try:
        response = requests.post(base_url, data=params, timeout=30)
        response.raise_for_status()
        
        # è§£æ XML
        root = ET.fromstring(response.content)
        articles = []
        
        for pubmed_article in root.findall(".//PubmedArticle"):
            try:
                medline = pubmed_article.find("MedlineCitation")
                article = medline.find("Article")
                
                # 1. åŸºç¡€ä¿¡æ¯
                pmid = medline.find("PMID").text
                title = article.find("ArticleTitle").text or "No Title"
                
                # 2. æ‘˜è¦æå–
                abstract_text = ""
                abstract = article.find("Abstract")
                if abstract is not None:
                    # æ‘˜è¦å¯èƒ½åˆ†æ®µï¼Œåˆå¹¶æ‰€æœ‰ AbstractText
                    texts = [elem.text for elem in abstract.findall("AbstractText") if elem.text]
                    abstract_text = " ".join(texts)
                
                # 3. ä½œè€…
                author_list = article.find("AuthorList")
                authors = []
                if author_list is not None:
                    for au in author_list.findall("Author"):
                        last = au.find("LastName")
                        initial = au.find("Initials")
                        name = ""
                        if last is not None: name += last.text
                        if initial is not None: name += " " + initial.text
                        if name: authors.append(name)
                
                author_str = ", ".join(authors[:3]) + ("..." if len(authors) > 3 else "")
                
                # 4. æ—¥æœŸ (å°è¯•è·å– PubDate)
                journal_issue = article.find("Journal/JournalIssue/PubDate")
                pub_date = "Unknown Date"
                if journal_issue is not None:
                    year = journal_issue.find("Year")
                    month = journal_issue.find("Month")
                    if year is not None:
                        pub_date = year.text
                        if month is not None: pub_date += f"-{month.text}"

                # 5. è®¡ç®—åŒ¹é…åº¦åˆ†æ•°
                # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œæ£€ç´¢
                full_text = (title + " " + abstract_text).lower()
                score = 0
                matched_keywords = []
                for kw in KEYWORDS_LIST:
                    if kw.lower() in full_text:
                        score += 1
                        matched_keywords.append(kw)
                
                article_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                
                articles.append({
                    "id": pmid,
                    "title": title,
                    "authors": author_str,
                    "date": pub_date,
                    "url": article_url,
                    "score": score,
                    "matches": matched_keywords
                })
                
            except Exception as e:
                # å•ä¸ªæ–‡ç« è§£æå¤±è´¥ä¸å½±å“æ•´ä½“
                continue

        # æ’åºï¼šä¼˜å…ˆæŒ‰åˆ†æ•°ï¼ˆé™åºï¼‰ï¼Œå…¶æ¬¡æŒ‰æ—¥æœŸï¼ˆå¦‚æœä¸è§„èŒƒåˆ™å¿½ç•¥ï¼‰ï¼Œæœ€ååŸåº
        # è¿™é‡Œä¸»è¦æŒ‰åˆ†æ•°é™åº
        articles.sort(key=lambda x: x["score"], reverse=True)
        return articles

    except Exception as e:
        print(f"[Error] è·å–/è§£ææ–‡çŒ®è¯¦æƒ…å¤±è´¥: {e}")
        return []

def send_feishu_card(webhook_url, keyword, articles):
    """
    å‘é€é£ä¹¦äº¤äº’å¼å¡ç‰‡æ¶ˆæ¯
    """
    if not articles:
        print("æ²¡æœ‰æ–‡ç« éœ€è¦å‘é€ã€‚")
        return

    # æ„å»ºå¡ç‰‡å†…å®¹
    elements = []
    for article in articles:
        # æ–‡ç« æ ‡é¢˜ + é“¾æ¥
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ“„ **[{article['title']}]({article['url']})**\nCorrelation Score: {article['score']} ğŸ”¥"
            }
        })
        # ä½œè€…å’Œæ—¥æœŸ
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ‘¤ ä½œè€…: {article['authors']}\nğŸ“… æ—¥æœŸ: {article['date']}"
            }
        })
        # åˆ†å‰²çº¿
        elements.append({"tag": "hr"})

    # é£ä¹¦å¡ç‰‡ç»“æ„
    card_content = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "template": "blue",
                "title": {
                    "content": f"ğŸ”¬ PubMed æœ€æ–°æ–‡çŒ®æ¨é€: {keyword}",
                    "tag": "plain_text"
                }
            },
            "elements": elements,
            "config": {
                "wide_screen_mode": True
            }
        }
    }

    try:
        response = requests.post(webhook_url, json=card_content)
        response.raise_for_status()
        res_json = response.json()
        if res_json.get("code") == 0:
            print("âœ… é£ä¹¦æ¶ˆæ¯å‘é€æˆåŠŸï¼")
        else:
            print(f"âŒ é£ä¹¦å‘é€å¤±è´¥: {res_json}")
    except Exception as e:
        print(f"[Error] Webhook è°ƒç”¨å¼‚å¸¸: {e}")

def main():
    print(f"ğŸš€ å¼€å§‹è¿è¡Œ PubMed ç›‘æ§è„šæœ¬...")
    print(f"ğŸ” å…³é”®è¯: {SEARCH_KEYWORD}")
    
    # 0. æ£€æŸ¥é…ç½®
    if "YOUR_FEISHU_WEBHOOK" in FEISHU_WEBHOOK_URL:
        print("âš ï¸  è­¦å‘Š: è¯·å…ˆåœ¨è„šæœ¬ä¸­é…ç½® 'FEISHU_WEBHOOK_URL' æ‰èƒ½å‘é€æ¶ˆæ¯ã€‚")
        print("   (æœ¬æ¬¡è¿è¡Œä»…ä¼šåœ¨æ§åˆ¶å°æ‰“å°ç»“æœ)")

    # 1. æ£€æŸ¥ä»Šæ—¥é¢åº¦
    history = load_history()
    today_count = history["count"]
    remaining_quota = DAILY_LIMIT - today_count
    
    print(f"ğŸ“… ä»Šæ—¥å·²æ¨é€: {today_count} ç¯‡, å‰©ä½™é¢åº¦: {remaining_quota} ç¯‡")
    
    if remaining_quota <= 0:
        print("ğŸš« ä»Šæ—¥é…é¢å·²ç”¨å®Œï¼Œåœæ­¢è¿è¡Œã€‚")
        return

    # 2. æœç´¢ ID (è·å–å¤šä¸€ç‚¹ä»¥ä¾¿æ’åº)
    ids = search_pubmed(SEARCH_KEYWORD, MAX_FETCH_RESULTS)
    if not ids:
        print("æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚")
        return
    
    # è¿‡æ»¤æ‰å·²ç»å‘é€è¿‡çš„ ID
    existing_ids = set(history["sent_ids"])
    new_ids = [uid for uid in ids if uid not in existing_ids]
    
    if not new_ids:
        print("æ‰€æœ‰æœç´¢åˆ°çš„æ–‡çŒ®å‡å·²æ¨é€è¿‡ã€‚")
        return

    print(f"âœ… æ‰¾åˆ° {len(new_ids)} ç¯‡æœªæ¨é€çš„å€™é€‰æ–‡çŒ®ï¼Œå‡†å¤‡è·å–è¯¦æƒ…å¹¶æ’åº...")

    # 3. è·å–è¯¦æƒ…å¹¶æ ¹æ®å…³é”®è¯åŒ¹é…åº¦æ’åº
    ranked_articles = get_details_and_rank(new_ids)
    
    # 4. æˆªå– Top N (ä¸è¶…è¿‡å‰©ä½™é…é¢)
    final_articles = ranked_articles[:remaining_quota]
    
    if not final_articles:
        print("æ²¡æœ‰å¯å‘é€çš„æ–‡ç« ã€‚")
        return

    print(f"ğŸ” ç²¾é€‰ Top {len(final_articles)} ç¯‡ (æŒ‰å…³é”®è¯åŒ¹é…åº¦):")
    for art in final_articles:
        print(f"   [{art['score']}pts] {art['title'][:50]}...")

    # 5. å‘é€ æˆ– æ‰“å°
    if "YOUR_FEISHU_WEBHOOK" in FEISHU_WEBHOOK_URL:
        # å¦‚æœæ²¡é…ç½® webhookï¼Œç›´æ¥æ‰“å°
        print("\n--- é¢„è§ˆæ¨¡å¼ (æœªé…ç½® Webhook) ---")
        for i, art in enumerate(final_articles, 1):
            print(f"{i}. [Score:{art['score']}] {art['title']}")
            print(f"   Link: {art['url']}\n")
    else:
        # å‘é€é£ä¹¦
        send_feishu_card(FEISHU_WEBHOOK_URL, " | ".join(KEYWORDS_LIST[:2])+"...", final_articles)
        
        # 6. æ›´æ–°å†å²è®°å½•
        history["count"] += len(final_articles)
        # å°†æ–°å‘é€çš„ ID åŠ å…¥å†å²ï¼Œé˜²æ­¢é‡å¤
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªä¿ç•™ IDï¼Œå¦‚æœæ–‡ä»¶è¿‡å¤§å¯ä»¥è€ƒè™‘æ¸…ç†æ—§ IDï¼Œç›®å‰æš‚ä¸å¤„ç†
        history["sent_ids"].extend([art["id"] for art in final_articles])
        save_history(history)
        print("ğŸ’¾ å†å²è®°å½•å·²æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()

